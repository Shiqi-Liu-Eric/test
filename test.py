# ------------------------------------------------------------
# 0. ç¯å¢ƒä¸è¾“å…¥ï¼šdf_id_map, ret1, date_list å·²é¢„åŠ è½½
# ------------------------------------------------------------
import pandas as pd
import numpy as np
from pathlib import Path
from pandas.tseries.offsets import BDay

from bmll2 import get_market_data_range           # Spark DF
import pyspark.sql.functions as F

OUT_DIR = Path("features")        # ä¿å­˜ç›®å½•
OUT_DIR.mkdir(exist_ok=True)

# ------------------------------------------------------------
# 1. é€‰å‡ºç¾å›½è‚¡ç¥¨è¡Œï¼Œå¹¶åˆ›å»ºæ‰€æœ‰ç©ºå£³ DF
# ------------------------------------------------------------
# 1.1 è¿‡æ»¤ index æœ«ä¸¤ä½ä¸º UN / UW
mask_us = df_id_map.index.str.endswith(("UN", "UW"))
tickers_us = df_id_map.index[mask_us]

# 1.2 åŸºå‡†å½¢çŠ¶ (è¡Œ = 8000+ ticker, åˆ— = ret1 çš„æ‰€æœ‰äº¤æ˜“æ—¥)
shape_base = ret1.copy(deep=False) * np.nan        # ä»…å ä½ï¼Œä¸å¤åˆ¶æ•°æ®
shape_base = shape_base.loc[tickers_us]            # ä»…ä¿ç•™ç¾è‚¡è¡Œ

# ç”¨å­—å…¸è£…æ‰€æœ‰ DF
raw_dfs = {}
sig_dfs = {}

# ------------------------------------------------------------
# 2. æ‰¹é‡æŠ“ L2 æ·±åº¦ â†’ Raw Features
# ------------------------------------------------------------
def fetch_l2_depth(mics, start, end):
    """ä¸€æ¬¡æ€§æŠ“å¤š marketã€å¤šæ—¥ L2ï¼Œå¹¶åœ¨ Spark å†…éƒ¨èšåˆä¸ºæ¯æ—¥æ¯è‚¡ç¥¨é¡¶æ¡£æ•°é‡å’Œä¸å¹³è¡¡"""
    spark_df = get_market_data_range(
        markets=mics,
        start_date=start.strftime('%Y-%m-%d'),
        end_date=end.strftime('%Y-%m-%d'),
        table="l2",
        columns=(["TradeDate", "Ticker"] +
                 [f"BidQuantity{i}" for i in range(1, 6)] +
                 [f"AskQuantity{i}" for i in range(1, 6)])
    )

    # Spark ä¾§è®¡ç®—äº”æ¡£åˆè®¡
    bid_sum = sum(F.col(f"BidQuantity{i}") for i in range(1, 6))
    ask_sum = sum(F.col(f"AskQuantity{i}") for i in range(1, 6))

    res = (spark_df
           .withColumn("VolBid5", bid_sum)
           .withColumn("VolAsk5", ask_sum)
           .withColumn("VolImb", (F.col("VolBid5") - F.col("VolAsk5")) /
                                 (F.col("VolBid5") + F.col("VolAsk5")))
           .groupBy("TradeDate", "Ticker")                 # æ—¥èšåˆ
           .agg(
               F.avg("VolBid5").alias("VolBid5"),
               F.avg("VolAsk5").alias("VolAsk5"),
               F.avg("VolImb").alias("VolImb")
           ))

    return res.toPandas()      # è¡Œæ•°â‰ˆ|ticker|Ã—|days|ï¼Œå·²è¶³å¤Ÿå°å¯è½¬ Pandas


# 2.1 å®šä¹‰æŠ“å–æ—¶é—´èŒƒå›´ï¼šret1.columns æœ€å°/æœ€å¤§
start_dt, end_dt = ret1.columns.min(), ret1.columns.max()
depth_df = fetch_l2_depth(["XNAS", "XNYS"], start_dt, end_dt)

# 2.2 è½¬æˆé€è§†è¡¨ï¼Œå¹¶å†™å…¥ raw_dfs ç©ºå£³
for col in ["VolBid5", "VolAsk5", "VolImb"]:
    tmp = depth_df.pivot(index="Ticker", columns="TradeDate", values=col)
    # å¯¹é½å½¢çŠ¶å¹¶å†™å…¥
    df_feat = shape_base.copy()
    df_feat.update(tmp)
    raw_dfs[col] = df_feat
    df_feat.to_pickle(OUT_DIR / f"raw_{col}.pkl")

# ğŸ‘‰ å¦‚éœ€é¢å¤– Raw Featureï¼Œä»¿ç…§ä¸Šé¢åœ¨ Spark é‡Œæ–°å¢åˆ—è®¡ç®—å³å¯
#    ä¾‹å¦‚ Spread_Depthã€HiddenRatio ç­‰ï¼Œç„¶åå† pivot/update

# ------------------------------------------------------------
# 3. è®¡ç®— Signal Featuresï¼ˆä»… T-5~T-1ï¼‰
# ------------------------------------------------------------
# 3.1 é¢„å…ˆç”Ÿæˆæ¯ä¸ª rebalance window çš„â€œå¾…è®¡ç®—æ—¥æœŸâ€é›†åˆ
windows = {eff: pd.bdate_range(eff - 5*BDay(), eff - BDay()) for eff in date_list}
all_need_dates = sorted(set.union(*map(set, windows.values())))

# 3.2 ä» raw feature ç›´æ¥è¡ç”Ÿä¿¡å· â€”â€” ä»¥ VolImb â†’ Q_Imb_Top5 ä¸ºä¾‹
#     å…¶ä»–ä¿¡å·ï¼ˆKyle_Lambda, MOC_Imb_RelADV, Spread_Depth...ï¼‰
#     å¯åœ¨ 2. çš„ Spark ä¸­ç®—å®Œåå†åš rolling / cross-sec ç­‰èšåˆ
def zscore_cross_sec(df_day):
    """æ¨ªæˆªé¢å»æå€¼(z)çš„å°å·¥å…·"""
    s = df_day.clip(lower=df_day.quantile(0.01), upper=df_day.quantile(0.99))
    return (s - s.mean()) / s.std(ddof=0)

sig_QImb = shape_base.copy()

for dt in all_need_dates:
    vec = raw_dfs["VolImb"][dt]             # ç³»åˆ—ï¼Œindex=ticker
    sig_QImb[dt] = zscore_cross_sec(vec)    # æ¨ªæˆªé¢ zscore

sig_dfs["QImb_Top5"] = sig_QImb
sig_QImb.to_pickle(OUT_DIR / "sig_QImb_Top5.pkl")

# ğŸ‘‰ è‹¥è¦é¢å¤–å› å­ï¼šKyle_Lambda, Auct_Dislc...
#    1. åœ¨ Raw é˜¶æ®µå¤šç®—å¯¹åº”åŸå§‹å­—æ®µæˆ–åˆ†é’Ÿæ¡
#    2. åœ¨æ­¤å¤„å†™ rolling / æ–œç‡ / äº¤äº’é¡¹é€»è¾‘
#    3. åŒæ · update è¿›å¯¹åº” sig_dfï¼Œpickle å³å¯

# ------------------------------------------------------------
# 4. ï¼ˆå¯é€‰ï¼‰ä¸€æ¬¡æ€§ä¿å­˜æ‰€æœ‰ DF
# ------------------------------------------------------------
for name, df in raw_dfs.items():
    df.to_pickle(OUT_DIR / f"raw_{name}.pkl")

for name, df in sig_dfs.items():
    df.to_pickle(OUT_DIR / f"sig_{name}.pkl")

print(f"âœ… å·²ç”Ÿæˆ {len(raw_dfs)} ä¸ª Raw & {len(sig_dfs)} ä¸ª Signal featureï¼Œä¿å­˜åœ¨ {OUT_DIR.resolve()}")
